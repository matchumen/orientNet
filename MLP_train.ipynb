{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_record(raw_record):\n",
    "    out_dict = {}\n",
    "    raw_data = raw_record.split(\";\")\n",
    "\n",
    "    out_dict[\"file_path\"] = raw_data[0]\n",
    "    tmp_keypoints = [data.split(\",\") for data in raw_data[1:9]]\n",
    "\n",
    "    out_dict[\"keypoints\"] = []\n",
    "    for keypoint in tmp_keypoints:\n",
    "        keypoint = [int(elem) for elem in keypoint]\n",
    "        out_dict[\"keypoints\"].append(keypoint)\n",
    "\n",
    "    out_dict[\"position\"] = [float(data) for data in raw_data[9:12]]\n",
    "\n",
    "    out_dict[\"rotation\"] = [float(data) for data in raw_data[12:]]\n",
    "\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointsDataset(Dataset):\n",
    "    def __init__(self, ann_file):\n",
    "        self.ann_file = ann_file\n",
    "        self.samples = []\n",
    "        with open(self.ann_file, 'r') as anns:\n",
    "            line = anns.readline()\n",
    "            while line:\n",
    "                sample = parse_record(line)\n",
    "                self.samples.append(sample)\n",
    "                line = anns.readline()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        input_keypoints = self.samples[idx]['keypoints']\n",
    "        input_vec = []\n",
    "        for keypoint in input_keypoints:\n",
    "            input_vec += keypoint\n",
    "        position = self.samples[idx]['position']\n",
    "        rotation = self.samples[idx]['rotation']\n",
    "        output_orientation = position + rotation\n",
    "        \n",
    "        input_vec = torch.as_tensor(np.array(input_vec), dtype=torch.float32)\n",
    "        \n",
    "        output_vec = torch.as_tensor(np.array(output_orientation), dtype=torch.float32)\n",
    "        \n",
    "        sample = {'input': input_vec, 'output': output_vec}\n",
    "        \n",
    "        return sample\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "with open('data/keypoints_data/annotations/ann.csv', 'r') as anns:\n",
    "    line = anns.readline()\n",
    "    while line:\n",
    "        sample = parse_record(line)\n",
    "        out = sample['position'] + sample['rotation']\n",
    "        inp = []\n",
    "        for keypoint in sample['keypoints']:\n",
    "            inp += keypoint\n",
    "        x_train.append(inp)\n",
    "        y_train.append(out)\n",
    "        line = anns.readline()\n",
    "                \n",
    "x_train = torch.as_tensor(x_train[:100], dtype=torch.float32)\n",
    "y_train = torch.as_tensor(y_train[:100], dtype=torch.float32)\n",
    "x_val = torch.as_tensor(x_train[-100:], dtype=torch.float32)\n",
    "y_val = torch.as_tensor(y_train[-100:], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = KeypointsDataset('data/keypoints_data/annotations/ann.csv')\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=2, num_workers=4)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=2, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features=input_dim, out_features=hidden_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(in_features=hidden_dim, out_features=hidden_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPNet(24,100,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl, lr_scheduler):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_data_loader:\n",
    "            loss_batch(model, loss_fn, xb, yb, opt)\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                losses, nums = zip(\n",
    "                *[loss_batch(model, loss_fn, xb, yb) for xb, yb in valid_dl]\n",
    "                )\n",
    "            val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print('Epoch: {}  Train loss: {:.2f}  Val loss: {:.2f}  Lr: {}'.format(\n",
    "            epoch, \n",
    "            loss, \n",
    "            val_loss, \n",
    "            opt.param_groups[0][\"lr\"]))   \n",
    "        lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = MLPNet(24,100,6)\n",
    "    return model, torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Train loss: 21.59  Val loss: 16.69  Lr: 1e-05\n",
      "Epoch: 1  Train loss: 22.11  Val loss: 16.65  Lr: 1e-05\n",
      "Epoch: 2  Train loss: 21.67  Val loss: 16.53  Lr: 1e-05\n",
      "Epoch: 3  Train loss: 21.39  Val loss: 16.60  Lr: 1e-05\n",
      "Epoch: 4  Train loss: 22.14  Val loss: 16.54  Lr: 1e-05\n",
      "Epoch: 5  Train loss: 21.64  Val loss: 16.50  Lr: 1e-05\n",
      "Epoch: 6  Train loss: 22.10  Val loss: 16.50  Lr: 1e-05\n",
      "Epoch: 7  Train loss: 21.33  Val loss: 16.43  Lr: 1e-05\n",
      "Epoch: 8  Train loss: 21.50  Val loss: 16.50  Lr: 1e-05\n",
      "Epoch: 9  Train loss: 21.26  Val loss: 16.37  Lr: 1e-05\n",
      "Epoch: 10  Train loss: 21.55  Val loss: 16.33  Lr: 1e-05\n",
      "Epoch: 11  Train loss: 20.89  Val loss: 16.40  Lr: 1e-05\n",
      "Epoch: 12  Train loss: 21.56  Val loss: 16.33  Lr: 1e-05\n",
      "Epoch: 13  Train loss: 20.66  Val loss: 16.32  Lr: 1e-05\n",
      "Epoch: 14  Train loss: 21.60  Val loss: 16.30  Lr: 1e-05\n",
      "Epoch: 15  Train loss: 20.93  Val loss: 16.20  Lr: 1e-05\n",
      "Epoch: 16  Train loss: 20.81  Val loss: 16.20  Lr: 1e-05\n",
      "Epoch: 17  Train loss: 21.24  Val loss: 16.21  Lr: 1e-05\n",
      "Epoch: 18  Train loss: 20.66  Val loss: 16.20  Lr: 1e-05\n",
      "Epoch: 19  Train loss: 20.95  Val loss: 16.14  Lr: 1e-05\n",
      "Epoch: 20  Train loss: 20.91  Val loss: 16.11  Lr: 1e-05\n",
      "Epoch: 21  Train loss: 20.57  Val loss: 16.08  Lr: 1e-05\n",
      "Epoch: 22  Train loss: 20.77  Val loss: 16.10  Lr: 1e-05\n",
      "Epoch: 23  Train loss: 20.28  Val loss: 16.04  Lr: 1e-05\n",
      "Epoch: 24  Train loss: 20.87  Val loss: 16.03  Lr: 1e-05\n",
      "Epoch: 25  Train loss: 20.20  Val loss: 15.98  Lr: 1e-05\n",
      "Epoch: 26  Train loss: 20.41  Val loss: 16.00  Lr: 1e-05\n",
      "Epoch: 27  Train loss: 20.34  Val loss: 15.97  Lr: 1e-05\n",
      "Epoch: 28  Train loss: 20.70  Val loss: 15.91  Lr: 1e-05\n",
      "Epoch: 29  Train loss: 19.83  Val loss: 15.88  Lr: 1e-05\n",
      "Epoch: 30  Train loss: 20.41  Val loss: 15.88  Lr: 1e-05\n",
      "Epoch: 31  Train loss: 20.49  Val loss: 15.85  Lr: 1e-05\n",
      "Epoch: 32  Train loss: 19.67  Val loss: 15.83  Lr: 1e-05\n",
      "Epoch: 33  Train loss: 20.16  Val loss: 15.75  Lr: 1e-05\n",
      "Epoch: 34  Train loss: 19.56  Val loss: 15.84  Lr: 1e-05\n",
      "Epoch: 35  Train loss: 20.26  Val loss: 15.77  Lr: 1e-05\n",
      "Epoch: 36  Train loss: 19.67  Val loss: 15.69  Lr: 1e-05\n",
      "Epoch: 37  Train loss: 19.91  Val loss: 15.72  Lr: 1e-05\n",
      "Epoch: 38  Train loss: 19.37  Val loss: 15.68  Lr: 1e-05\n",
      "Epoch: 39  Train loss: 20.00  Val loss: 15.65  Lr: 1e-05\n",
      "Epoch: 40  Train loss: 19.98  Val loss: 15.66  Lr: 1e-05\n",
      "Epoch: 41  Train loss: 19.58  Val loss: 15.61  Lr: 1e-05\n",
      "Epoch: 42  Train loss: 19.04  Val loss: 15.62  Lr: 1e-05\n",
      "Epoch: 43  Train loss: 19.57  Val loss: 15.56  Lr: 1e-05\n",
      "Epoch: 44  Train loss: 19.45  Val loss: 15.52  Lr: 1e-05\n",
      "Epoch: 45  Train loss: 18.79  Val loss: 15.54  Lr: 1e-05\n",
      "Epoch: 46  Train loss: 19.94  Val loss: 15.50  Lr: 1e-05\n",
      "Epoch: 47  Train loss: 19.67  Val loss: 15.48  Lr: 1e-05\n",
      "Epoch: 48  Train loss: 18.96  Val loss: 15.44  Lr: 1e-05\n",
      "Epoch: 49  Train loss: 19.15  Val loss: 15.41  Lr: 1e-05\n",
      "Epoch: 50  Train loss: 19.15  Val loss: 15.40  Lr: 1e-05\n",
      "Epoch: 51  Train loss: 19.33  Val loss: 15.40  Lr: 1e-05\n",
      "Epoch: 52  Train loss: 18.62  Val loss: 15.34  Lr: 1e-05\n",
      "Epoch: 53  Train loss: 18.67  Val loss: 15.38  Lr: 1e-05\n",
      "Epoch: 54  Train loss: 19.21  Val loss: 15.31  Lr: 1e-05\n",
      "Epoch: 55  Train loss: 19.26  Val loss: 15.28  Lr: 1e-05\n",
      "Epoch: 56  Train loss: 18.95  Val loss: 15.27  Lr: 1e-05\n",
      "Epoch: 57  Train loss: 18.83  Val loss: 15.22  Lr: 1e-05\n",
      "Epoch: 58  Train loss: 18.69  Val loss: 15.26  Lr: 1e-05\n",
      "Epoch: 59  Train loss: 18.99  Val loss: 15.22  Lr: 1e-05\n",
      "Epoch: 60  Train loss: 18.63  Val loss: 15.19  Lr: 1e-05\n",
      "Epoch: 61  Train loss: 18.65  Val loss: 15.16  Lr: 1e-05\n",
      "Epoch: 62  Train loss: 18.79  Val loss: 15.11  Lr: 1e-05\n",
      "Epoch: 63  Train loss: 18.62  Val loss: 15.11  Lr: 1e-05\n",
      "Epoch: 64  Train loss: 17.97  Val loss: 15.07  Lr: 1e-05\n",
      "Epoch: 65  Train loss: 18.67  Val loss: 15.06  Lr: 1e-05\n",
      "Epoch: 66  Train loss: 18.38  Val loss: 15.11  Lr: 1e-05\n",
      "Epoch: 67  Train loss: 18.72  Val loss: 15.03  Lr: 1e-05\n",
      "Epoch: 68  Train loss: 18.40  Val loss: 14.99  Lr: 1e-05\n",
      "Epoch: 69  Train loss: 17.98  Val loss: 15.01  Lr: 1e-05\n",
      "Epoch: 70  Train loss: 18.77  Val loss: 14.97  Lr: 1e-05\n",
      "Epoch: 71  Train loss: 17.87  Val loss: 14.94  Lr: 1e-05\n",
      "Epoch: 72  Train loss: 18.44  Val loss: 14.91  Lr: 1e-05\n",
      "Epoch: 73  Train loss: 18.19  Val loss: 14.94  Lr: 1e-05\n",
      "Epoch: 74  Train loss: 17.78  Val loss: 14.90  Lr: 1e-05\n",
      "Epoch: 75  Train loss: 18.13  Val loss: 14.84  Lr: 1e-05\n",
      "Epoch: 76  Train loss: 18.36  Val loss: 14.83  Lr: 1e-05\n",
      "Epoch: 77  Train loss: 17.95  Val loss: 14.79  Lr: 1e-05\n",
      "Epoch: 78  Train loss: 17.75  Val loss: 14.85  Lr: 1e-05\n",
      "Epoch: 79  Train loss: 17.86  Val loss: 14.77  Lr: 1e-05\n",
      "Epoch: 80  Train loss: 17.52  Val loss: 14.77  Lr: 1e-05\n",
      "Epoch: 81  Train loss: 17.95  Val loss: 14.73  Lr: 1e-05\n",
      "Epoch: 82  Train loss: 17.62  Val loss: 14.69  Lr: 1e-05\n",
      "Epoch: 83  Train loss: 17.69  Val loss: 14.68  Lr: 1e-05\n",
      "Epoch: 84  Train loss: 17.97  Val loss: 14.67  Lr: 1e-05\n",
      "Epoch: 85  Train loss: 17.03  Val loss: 14.59  Lr: 1e-05\n",
      "Epoch: 86  Train loss: 17.28  Val loss: 14.68  Lr: 1e-05\n",
      "Epoch: 87  Train loss: 17.84  Val loss: 14.60  Lr: 1e-05\n",
      "Epoch: 88  Train loss: 16.99  Val loss: 14.63  Lr: 1e-05\n",
      "Epoch: 89  Train loss: 17.84  Val loss: 14.56  Lr: 1e-05\n",
      "Epoch: 90  Train loss: 17.43  Val loss: 14.54  Lr: 1e-05\n",
      "Epoch: 91  Train loss: 17.07  Val loss: 14.59  Lr: 1e-05\n",
      "Epoch: 92  Train loss: 17.59  Val loss: 14.51  Lr: 1e-05\n",
      "Epoch: 93  Train loss: 17.48  Val loss: 14.50  Lr: 1e-05\n",
      "Epoch: 94  Train loss: 16.98  Val loss: 14.46  Lr: 1e-05\n",
      "Epoch: 95  Train loss: 17.45  Val loss: 14.43  Lr: 1e-05\n",
      "Epoch: 96  Train loss: 16.85  Val loss: 14.36  Lr: 1e-05\n",
      "Epoch: 97  Train loss: 17.19  Val loss: 14.39  Lr: 1e-05\n",
      "Epoch: 98  Train loss: 17.33  Val loss: 14.37  Lr: 1e-05\n",
      "Epoch: 99  Train loss: 16.79  Val loss: 14.39  Lr: 1e-05\n"
     ]
    }
   ],
   "source": [
    "#train_dl, valid_dl = get_data(train_dataset, val_dataset, 100)\n",
    "#model, opt = get_model()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "fit(100, model, loss_fn, opt, train_dl, valid_dl, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.800  0.570  19.250  57.990  346.560  55.690]\n",
      "[ 0.691  0.316  18.363  59.384  343.465  54.969]\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, len(val_dataset)-1)\n",
    "pred = model(val_dataset[idx][0]).detach().numpy()\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "print(val_dataset[idx][1].numpy())\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mlp_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
